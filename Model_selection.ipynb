{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import pi\n",
    "\n",
    "# spot check on engineered-features\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_raw():\n",
    "    X = pd.read_csv('Data/X_train.csv')\n",
    "    y = pd.read_csv('Data/y_train.csv')\n",
    "    return (X, y)\n",
    "\n",
    "def get_values_X_2D(X):\n",
    "    n_timestep, n_feature = 128, 10\n",
    "    X_data_ori = X.iloc[:,3:].values # original values\n",
    "    X_data = StandardScaler().fit_transform(X_data_ori)# standarlize the data\n",
    "    #X_data = MinMaxScaler().fit_transform(X_data)# standarlize the data\n",
    "\n",
    "    X_data_2D = np.reshape(X_data, (int(len(X_data)/n_timestep),n_timestep,n_feature)) # convert to 2D images\n",
    "    return (X_data_2D)\n",
    "\n",
    "\n",
    "def get_data(flatten = False): \n",
    "    # intend format, X = (n_example, n_timestep*n_features) when flatten is true\n",
    "    # otherwise, X = (n_example, n_timestep, n_features)\n",
    "    # y = surface name \n",
    "    X, y = load_data_raw()\n",
    "    X_data = get_values_X_2D(X)\n",
    "    if flatten == True:\n",
    "        X_data = np.reshape(X_data, (np.shape(X_data)[0], np.shape(X_data)[1]*np.shape(X_data)[2]))\n",
    "\n",
    "    y_data = y.surface\n",
    "    print (np.shape(X_data))\n",
    "    print (np.shape(y_data))\n",
    "    return (X_data, y_data)\n",
    "    \n",
    "    \n",
    "\n",
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "    # nonlinear models\n",
    "    models['knn'] = KNeighborsClassifier(n_neighbors=7)\n",
    "    models['cart'] = DecisionTreeClassifier()\n",
    "    models['svm'] = SVC()\n",
    "    models['bayes'] = GaussianNB()\n",
    "    # ensemble models\n",
    "    models['bag'] = BaggingClassifier(n_estimators=100)\n",
    "    models['rf'] = RandomForestClassifier(n_estimators=100)\n",
    "    models['et'] = ExtraTreesClassifier(n_estimators=100)\n",
    "    models['gbm'] = GradientBoostingClassifier(n_estimators=100)\n",
    "    print('Defined %d models' % len(models))\n",
    "    return models\n",
    " \n",
    "# evaluate a single model\n",
    "def evaluate_model(trainX, trainy, testX, testy, model):\n",
    "    # fit the model\n",
    "    model.fit(trainX, trainy)\n",
    "    # make predictions\n",
    "    yhat = model.predict(testX)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(testy, yhat)\n",
    "    return accuracy * 100.0\n",
    " \n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(trainX, trainy, testX, testy, models):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        # evaluate the model\n",
    "        results[name] = evaluate_model(trainX, trainy, testX, testy, model)\n",
    "        # show process\n",
    "        print('>%s: %.3f' % (name, results[name]))\n",
    "    return results\n",
    " \n",
    "# print and plot the results\n",
    "def summarize_results(results, maximize=True):\n",
    "    # create a list of (name, mean(scores)) tuples\n",
    "    mean_scores = [(k,v) for k,v in results.items()]\n",
    "    # sort tuples by mean score\n",
    "    mean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "    # reverse for descending order (e.g. for accuracy)\n",
    "    if maximize:\n",
    "        mean_scores = list(reversed(mean_scores))\n",
    "    print()\n",
    "    for name, score in mean_scores:\n",
    "        print('Name=%s, Score=%.3f' % (name, score))\n",
    "\n",
    "def conv_angles(x,y,z,w):\n",
    "    t0 = +2.0 * (w * x + y * z)\n",
    "    t1 = +1.0 - 2.0 * (x * x + y * y)\n",
    "\n",
    "    X = np.arctan2(t0, t1)\n",
    "\n",
    "    t2 = +2.0 * (w * y - z * x)\n",
    "    index_out = np.where(np.absolute(t2)>=1)# get the index which is out of range\n",
    "    sign_out = np.sign(t2) # if out of range change it to pi/2 or -pi/2\n",
    "    t2[index_out] = t2[index_out] *sign_out[index_out] \n",
    "    Y = np.arcsin(t2)\n",
    "    \n",
    "\n",
    "    t3 = +2.0 * (w * z + x * y)\n",
    "    t4 = +1.0 - 2.0 * (y * y + z * z)\n",
    "    Z = np.arctan2(t3, t4)\n",
    "    \n",
    "\n",
    "    return X, Y, Z\n",
    "    \n",
    "def conv_euler_angles(X_pd): # input is the panda dataframe of X\n",
    "    n_timestep, n_feature = 128, 10\n",
    "    X_data_ori = X_pd.iloc[:,3:].values # original values\n",
    "    n_example = int(np.shape(X_data_ori)[0]/n_timestep)\n",
    "\n",
    "    new_feature= np.zeros((n_example, n_timestep, 3))\n",
    "    X = np.reshape(X_data_ori, (n_example, n_timestep, n_feature))\n",
    "    for id_n in range(n_example):\n",
    "        x = X[id_n,:,0] # x\n",
    "        y = X[id_n,:,1] # y\n",
    "        z = X[id_n,:,2] # z\n",
    "        w = X[id_n,:,3] # w\n",
    "        \n",
    "        new_feature[id_n,:,0], new_feature[id_n,:,1], new_feature[id_n,:,2] = conv_angles(x,y,z,w) \n",
    "\n",
    "        \n",
    "    new_feature_norm_1d= np.reshape(new_feature,(n_example*n_timestep, 3))\n",
    "    #new_feature_norm_1d = StandardScaler().fit_transform(new_feature_norm_1d)# standarlize the data\n",
    "    \n",
    "    new_feature_norm_2d = np.reshape(new_feature_norm_1d,(n_example, n_timestep, 3))\n",
    "    X_values_comb = np.concatenate((X, new_feature_norm_2d), axis=2)\n",
    "\n",
    "    return(X_values_comb)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 804  826  830 ... 3807 3808 3809] TEST: [   0    1    2 ... 2223 2225 2230]\n",
      "Defined 8 models\n",
      ">knn: 17.453\n",
      ">cart: 30.294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">svm: 34.015\n",
      ">bayes: 27.830\n",
      ">bag: 32.495\n",
      ">rf: 31.132\n",
      ">et: 39.413\n",
      ">gbm: 38.260\n",
      "\n",
      "Name=et, Score=39.413\n",
      "Name=gbm, Score=38.260\n",
      "Name=svm, Score=34.015\n",
      "Name=bag, Score=32.495\n",
      "Name=rf, Score=31.132\n",
      "Name=cart, Score=30.294\n",
      "Name=bayes, Score=27.830\n",
      "Name=knn, Score=17.453\n",
      "TRAIN: [   0    1    2 ... 2223 2225 2230] TEST: [ 804  826  830 ... 3807 3808 3809]\n",
      "Defined 8 models\n",
      ">knn: 20.768\n",
      ">cart: 37.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">svm: 28.601\n",
      ">bayes: 17.823\n",
      ">bag: 41.693\n",
      ">rf: 43.323\n",
      ">et: 47.056\n",
      ">gbm: 45.058\n",
      "\n",
      "Name=et, Score=47.056\n",
      "Name=gbm, Score=45.058\n",
      "Name=rf, Score=43.323\n",
      "Name=bag, Score=41.693\n",
      "Name=cart, Score=37.802\n",
      "Name=svm, Score=28.601\n",
      "Name=knn, Score=20.768\n",
      "Name=bayes, Score=17.823\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=2)\n",
    "StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in skf.split(X_data, y_data):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X_data[train_index,:], X_data[test_index,:]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    models = define_models()\n",
    "    # evaluate models\n",
    "    results = evaluate_models(X_train, y_train, X_test, y_test, models)\n",
    "    # summarize results\n",
    "    summarize_results(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
